{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline # Not strictly used for final SM model but good for local\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb # For local model training and understanding\n",
    "\n",
    "# SageMaker specific imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.xgboost.estimator import XGBoost as SageMakerXGBoost # Alias to avoid confusion\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "import os # For joining paths\n",
    "import time # For unique endpoint names\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('telco-customer-churn.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Initial dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'telco-customer-churn.csv' not found. Make sure the file is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "print(f\"\\nMissing values in 'TotalCharges' after conversion to numeric: {df['TotalCharges'].isnull().sum()}\")\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "    print(\"Dropped 'customerID' column.\")\n",
    "if df['TotalCharges'].isnull().any():\n",
    "    median_total_charges = df[df['tenure'] > 0]['TotalCharges'].median() # Median from customers with tenure\n",
    "    if pd.isna(median_total_charges) and df['TotalCharges'].isnull().sum() > 0 : # if all are new customers\n",
    "         median_total_charges = 0 # Fallback if all tenures are 0 or only NaNs exist\n",
    "    df['TotalCharges'].fillna(median_total_charges, inplace=True)\n",
    "    print(f\"Imputed missing 'TotalCharges' with median: {median_total_charges}\")\n",
    "print(f\"Missing values in 'TotalCharges' after imputation: {df['TotalCharges'].isnull().sum()}\")\n",
    "if 'Churn' in df.columns:\n",
    "    if df['Churn'].dtype == 'object': # Check if it needs encoding\n",
    "        label_encoder_churn = LabelEncoder()\n",
    "        df['Churn'] = label_encoder_churn.fit_transform(df['Churn'])\n",
    "        print(\"'Churn' column encoded to numerical (0 and 1).\")\n",
    "        # To see the mapping: print(dict(zip(label_encoder_churn.classes_, label_encoder_churn.transform(label_encoder_churn.classes_))))\n",
    "else:\n",
    "    print(\"Error: 'Churn' column not found. Cannot proceed with target encoding.\")\n",
    "    exit()\n",
    "if 'Churn' in df.columns:\n",
    "    if df['Churn'].dtype == 'object': # Check if it needs encoding\n",
    "        label_encoder_churn = LabelEncoder()\n",
    "        df['Churn'] = label_encoder_churn.fit_transform(df['Churn'])\n",
    "        print(\"'Churn' column encoded to numerical (0 and 1).\")\n",
    "        # To see the mapping: print(dict(zip(label_encoder_churn.classes_, label_encoder_churn.transform(label_encoder_churn.classes_))))\n",
    "else:\n",
    "    print(\"Error: 'Churn' column not found. Cannot proceed with target encoding.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)  # Assuming 'Churn' is the target column\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train and test data to CSV\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "test_data.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload data to S3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'telco-churn-xgboost'\n",
    "\n",
    "train_s3_path = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)\n",
    "test_s3_path = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f'Train data uploaded to: {train_s3_path}')\n",
    "print(f'Test data uploaded to: {test_s3_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "# Define the XGBoost model\n",
    "role = get_execution_role()\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker_session.boto_region_name, \"1.5-1\")\n",
    "\n",
    "# Use SageMaker's built-in XGBoost algorithm\n",
    "xgboost_estimator = sagemaker.estimator.Estimator(\n",
    "    xgboost_container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgboost_estimator.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='auc'\n",
    ")   \n",
    "\n",
    "# Prepare the training input\n",
    "train_input = TrainingInput(train_s3_path, content_type=\"csv\")\n",
    "test_input = TrainingInput(test_s3_path, content_type=\"csv\")\n",
    "\n",
    "xgboost_estimator.fit({'train': train_input, 'validation': test_input})\n",
    "\n",
    "# get the model artifact\n",
    "model_artifact = xgboost_estimator.model_data\n",
    "print(f\"Model artifact saved at: {model_artifact}\")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08698ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "xgboost_model = sagemaker.model.Model(\n",
    "    model_data=model_artifact,\n",
    "    image_uri=xgboost_container,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    predictor_cls=sagemaker.predictor.Predictor\n",
    ")\n",
    "\n",
    "#get the model artifact\n",
    "model_artifact = xgboost_model.model_data\n",
    "print(f\"Model artifact saved at: {model_artifact}\")\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "endpoint_name = f\"xgboost-telco-churn-{int(time.time())}\"\n",
    "\n",
    "predictor = xgboost_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687530f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the endpoint using the enpdpoint name and the test data\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "# Prepare a sample payload for prediction\n",
    "sample_data = X_test.iloc[:1].to_csv(header=False, index=False)\n",
    "\n",
    "# Print the sample data for debugging\n",
    "print(f\"Sample data for prediction: {sample_data}\")\n",
    "\n",
    "# Convert the sample data to numerical format and ensure it matches the expected input format\n",
    "sample_data = X_test.iloc[:1].apply(pd.to_numeric, errors='coerce').to_csv(header=False, index=False)\n",
    "\n",
    "# Make a prediction\n",
    "predicted = predictor.predict(sample_data, initial_args={'ContentType': 'text/csv'})\n",
    "print(f\"Predicted value: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of numerical and categorical features from the training dataset\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Numerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "import json\n",
    "\n",
    "# Create a dictionary to store the metadata\n",
    "metadata = {\n",
    "   \"predictorList\": []\n",
    "}\n",
    "\n",
    "# add a model object to the metadata with the following keys objective , outcomeType, expectedPerformance , framework and modellingtechnique\n",
    "metadata[\"model\"] = {\n",
    "    \"objective\": \"Churn\",\n",
    "    \"outcomeType\": \"BINARY\",\n",
    "    \"expectedPerformance\": 70,\n",
    "    \"expectedPerformanceMeasurement\": \"AUC\",\n",
    "    \"framework\": \"xgboost\",\n",
    "    \"modellingTechnique\": \"XGBoost\",\n",
    "    # add a outcomes object which has a range array of 0 to 1\n",
    "    \"outcomes\": {\n",
    "         \"range\": []\n",
    "     }\n",
    "}\n",
    "\n",
    "# Add numerical features to the metadata\n",
    "for feature in numerical_features:\n",
    "    metadata[\"predictorList\"].append({\"name\": feature, \"type\": \"NUMERIC\"})\n",
    "\n",
    "# Add categorical features to the metadata\n",
    "for feature in categorical_features:\n",
    "    metadata[\"predictorList\"].append({\"name\": feature, \"type\": \"CATEGORICAL\"})\n",
    "\n",
    "# Save the metadata to a JSON file\n",
    "with open(\"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"Model metadata saved to 'model_metadata.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
