{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline # Not strictly used for final SM model but good for local\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb # For local model training and understanding\n",
    "\n",
    "# SageMaker specific imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.xgboost.estimator import XGBoost as SageMakerXGBoost # Alias to avoid confusion\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "import os # For joining paths\n",
    "import time # For unique endpoint names\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('telco-customer-churn.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Initial dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'telco-customer-churn.csv' not found. Make sure the file is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "print(f\"\\nMissing values in 'TotalCharges' after conversion to numeric: {df['TotalCharges'].isnull().sum()}\")\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "    print(\"Dropped 'customerID' column.\")\n",
    "if df['TotalCharges'].isnull().any():\n",
    "    median_total_charges = df[df['tenure'] > 0]['TotalCharges'].median() # Median from customers with tenure\n",
    "    if pd.isna(median_total_charges) and df['TotalCharges'].isnull().sum() > 0 : # if all are new customers\n",
    "         median_total_charges = 0 # Fallback if all tenures are 0 or only NaNs exist\n",
    "    df['TotalCharges'].fillna(median_total_charges, inplace=True)\n",
    "    print(f\"Imputed missing 'TotalCharges' with median: {median_total_charges}\")\n",
    "print(f\"Missing values in 'TotalCharges' after imputation: {df['TotalCharges'].isnull().sum()}\")\n",
    "if 'Churn' in df.columns:\n",
    "    if df['Churn'].dtype == 'object': # Check if it needs encoding\n",
    "        label_encoder_churn = LabelEncoder()\n",
    "        df['Churn'] = label_encoder_churn.fit_transform(df['Churn'])\n",
    "        print(\"'Churn' column encoded to numerical (0 and 1).\")\n",
    "        # To see the mapping: print(dict(zip(label_encoder_churn.classes_, label_encoder_churn.transform(label_encoder_churn.classes_))))\n",
    "else:\n",
    "    print(\"Error: 'Churn' column not found. Cannot proceed with target encoding.\")\n",
    "    exit()\n",
    "if 'Churn' in df.columns:\n",
    "    if df['Churn'].dtype == 'object': # Check if it needs encoding\n",
    "        label_encoder_churn = LabelEncoder()\n",
    "        df['Churn'] = label_encoder_churn.fit_transform(df['Churn'])\n",
    "        print(\"'Churn' column encoded to numerical (0 and 1).\")\n",
    "        # To see the mapping: print(dict(zip(label_encoder_churn.classes_, label_encoder_churn.transform(label_encoder_churn.classes_))))\n",
    "else:\n",
    "    print(\"Error: 'Churn' column not found. Cannot proceed with target encoding.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)  # Assuming 'Churn' is the target column\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train and test data to CSV\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "test_data.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload data to S3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'telco-churn-xgboost'\n",
    "\n",
    "train_s3_path = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)\n",
    "test_s3_path = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f'Train data uploaded to: {train_s3_path}')\n",
    "print(f'Test data uploaded to: {test_s3_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "# Define the XGBoost model\n",
    "role = get_execution_role()\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker_session.boto_region_name, \"1.5-1\")\n",
    "\n",
    "# Use SageMaker's built-in XGBoost algorithm\n",
    "xgboost_estimator = sagemaker.estimator.Estimator(\n",
    "    xgboost_container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgboost_estimator.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='auc'\n",
    ")   \n",
    "\n",
    "# Prepare the training input\n",
    "train_input = TrainingInput(train_s3_path, content_type=\"csv\")\n",
    "test_input = TrainingInput(test_s3_path, content_type=\"csv\")\n",
    "\n",
    "\n",
    "xgboost_estimator.fit({'train': train_input, 'validation': test_input})\n",
    "\n",
    "# get the model artifact\n",
    "model_artifact = xgboost_estimator.model_data\n",
    "print(f\"Model artifact saved at: {model_artifact}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08698ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "xgboost_model = xgboost_estimator.create_model(\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "#get the model artifact\n",
    "model_artifact = xgboost_model.model_data\n",
    "print(f\"Model artifact saved at: {model_artifact}\")\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "endpoint_name = f\"xgboost-telco-churn-{int(time.time())}\"\n",
    "predictor = xgboost_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer()\n",
    ")\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f934f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the start time for the endpoint creation\n",
    "import time\n",
    "start_time = time.time()\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(start_time))\n",
    "print(f\"Endpoint creation started at: {start_time}\")\n",
    "\n",
    "# Create an endpoint name call it telco-churn-xgboost-endpoint and add a timestamp\n",
    "\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")    \n",
    "endpoint_name = f'telco-churn-xgboost-endpoint-{timestamp}'\n",
    "\n",
    "# Deploy the model\n",
    "xgboost_predictor = xgboost_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "# Print the endpoint creation time\n",
    "end_time = time.time() \n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(end_time))\n",
    "print(f\"Endpoint creation completed at: {end_time}\")\n",
    "print(f\"Endpoint {endpoint_name} is ready to serve predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49361bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a sample payload for prediction\n",
    "sample_data = X_test.iloc[:1].to_csv(header=False, index=False)\n",
    "\n",
    "# Convert the sample data to numerical format and ensure it matches the expected input format\n",
    "sample_data = X_test.iloc[:1].apply(pd.to_numeric, errors='coerce').to_csv(header=False, index=False)\n",
    "\n",
    "# Print the sample data\n",
    "print(\"Sample data for prediction:\")\n",
    "print(sample_data)\n",
    "sample_data = sample_data.encode('utf-8')\n",
    "# Print the encoded sample data\n",
    "print(\"Encoded sample data:\")       \n",
    "print(sample_data)\n",
    "\n",
    "# Invoke the endpoint with the correct content type\n",
    "response = xgboost_predictor.predict(sample_data, initial_args={\"ContentType\": \"text/csv\"})\n",
    "\n",
    "# Decode the response\n",
    "print(\"Predictions:\")\n",
    "#print(response.decode('utf-8'))\n",
    "print(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of numerical and categorical features from the training dataset\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Numerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "import json\n",
    "\n",
    "# Create a dictionary to store the metadata\n",
    "metadata = {\n",
    "    \"model_name\": \"Telco Churn XGBoost\",\n",
    "    \"model_version\": \"1.0\",\n",
    "    \"description\": \"XGBoost model for predicting customer churn\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Add numerical features to the metadata\n",
    "for feature in numerical_features:\n",
    "    metadata[\"features\"].append({\"name\": feature, \"type\": \"numerical\"})\n",
    "\n",
    "# Add categorical features to the metadata\n",
    "for feature in categorical_features:\n",
    "    metadata[\"features\"].append({\"name\": feature, \"type\": \"categorical\"})\n",
    "\n",
    "# Save the metadata to a JSON file\n",
    "with open(\"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"Model metadata saved to 'model_metadata.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
